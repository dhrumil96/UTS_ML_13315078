{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "ML_A1_13315078.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhrumil96/UTS_ML_13315078/blob/master/ML_A1_13315078.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rnShogaB-FE",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Support-Vector Networks\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-_JIjdbB-FG",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ovtTGrOyoC8",
        "colab_type": "text"
      },
      "source": [
        "Assignment 1 of Machine Learning \n",
        "\n",
        "Created By - Dhrumil Patel (Student ID:13315078)\n",
        "\n",
        "This file is available on the provided github link\n",
        "https://github.com/dhrumil96/UTS_ML_13315078/blob/master/ML_A1_13315078.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAzFlBL1B-FH",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NlupxbeB-FI",
        "colab_type": "text"
      },
      "source": [
        "Support Vector Network is used to classified two group classification problem in which input vectors are non-linearly mapped to a very high dimension feature space. Already support vector netwok was just ready to group in which training datasets can be isolated however this paper extend to classify non-separable dataset.\n",
        "\n",
        "Paper show the how better it sums up for high degree polynomial choice surfaces up to 7 in high dimensional space.\n",
        "\n",
        "In the given procedure there are two main issues. One of the conceptual issue was calculated that was the means by which to seperate the hyperplane that can sum up better in high dimensionality in feature space. Arrangement of this issue was settled in 1965 which is spot result of the vector with its relating weight will frame the linear combination of the suppport vector. Another issue was technical that was how to handle high polynomial degree in feature space. And solution for the technical issue was given by the (Boser et al, 1992) that instead of doing dot-product of support vectors to generate non-linear transformation one can compare two vectors such as measuring some distance this helps to build better classes of decision surfaces.\n",
        "\n",
        "As stated above that this paper extend to classify non separable training data for that one method introduced which is known as soft margin hyperplane. In that method one can add some non-negative variables to minimize the errors. And for the feature space method of convolution of the dot product mentioned in the paper which transform n-dimensional input vectors into N-dimensional feature vector.\n",
        "\n",
        "Some of the experiements given in the paper which are experiement on the plane and digit recognition. For that its results are given in the paper which justify the support vector network machine learning gives the better result compare to decision tree, linear classifier and k-nearest neighbour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvBD89-vB-FK",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrUgKXjVB-FM",
        "colab_type": "text"
      },
      "source": [
        "In 1936 Fisher introduced algorithm for the pattern recognition using two normal distributed populations, N(m1, E1) and N(m2, E2) of n dimensional vectors x with mean vectors m1 and m2 and co-varience matrices E1 and E2. When co-varience are equal that time two distributions are not normal for that he introduced new function which consist of constant. From starting this was related to development of linear choice surface.\n",
        "\n",
        "Different type of learning machine was invented by Rosenblatt in 1962 which was based on perceptrons. Basically perceptron are linked to the neurons and those neuron separate the hyperplane. At last perceptron implement the piecewise linear separating surface.\n",
        "\n",
        "All weights of neural network are accepted in order locally to reduce the error on the set of vectors belonging to pattern recognition problem was found when the back propogation algorithm was discovered. Effective changes in neural network implements piece-wise-linear-type decision functions.\n",
        "\n",
        "In the given article it recommend another kind of learning machine called Support-vector Network. Utilizing chosen non-linear mapping of input vectors into high dimensional feature space Z group on the training dataset this machine learning known as support vector network. In this space a linear surface is built with extraordinary properties that guarantee high speculation capacity of the system. In the paper it has shown optimal hyperplane algorithm for separation of the training data without error. Even before this paper writtern support vector was only able to classify in two groups without erros but in this paper suggested method of Soft Margin Hyperplane allows to training data can not be seperated without error. Method of Convolution of the dot product in feature space suggests a vector function which can transform the n-dimensional input vector x into an N-dimensional feature vector. In this convolution of dot product, according to the Hilbert-Schmidt theory every symmetric function can be transformed to the integral operator defined by kernal k(u,v). To provide co-efficient are positive it is necessary to satisfy  Mercer's theorem. Using various dot product function anyone can create machine learning with arbitary types of decision surface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUexxx9qB-FN",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUiilxHFB-FO",
        "colab_type": "text"
      },
      "source": [
        "Overall the paper has a high level of technical quality. The core logic of Support Vector Network is explained in detail and supported by mathematical equations which allow other reasearchers to gain some understanding how support vector network works. Step by step explanation of all the terms is given very well which helps the readers to understand easily. Also in technical terms of mathematical equations of optimal hyperplane, soft margin hyperplane explained much better using both of the cases where number of error are minimum and maximum along with its benefits and drawbacks and convolution of the dot-product is explained very well in detail by suggesting different functions were invented by some authors.\n",
        "\n",
        "However, there are some problems that lower the technical quality of paper. Problem can be considered as the very few experiement or very similar experiements shown in the paper such as the there are two same kind of experiements done in the digit recognition pattern using two different datasets. Along with mathematical equations of suggested methods author should provide some logical or graphical representation of the explained logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lx_eN7DB-FP",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-GmWKvBgPfhD"
      },
      "source": [
        "To consider the application of the support vector network, in this paper has proposed the two types of the experiements one of the experiement is construct artificial sets of patterns in the plane and experiements with the 2nd degree polynomial decision surfaces and second experiement with real life problem of digit recognition.\n",
        "\n",
        "In the first experiement, result of the experiement visualized and provide nice illustrations of the power of the algorithm. Two class represented by white and black bullets.\n",
        "\n",
        "Second experiement was performed using two different databases for bit mapped digit recognition,a small and a large database. Small one is a US postal service database that contains 7300 training pattern and 2000 test patterns. The large database contains 60000 training and 10000 test patterns of the NIST organization. \n",
        "\n",
        "\n",
        "*   After perfoming the various methods on the small database training patterns got the error rate 17% in decision tree CART5, 16% decision tree C4.5 and in support vector network 5.1 which was the lowest error rate among all experiements.\n",
        "\n",
        "*   In large database of NIST resulted error rate as below\n",
        "\n",
        "   Linear classifier     8.4   \n",
        "   K=3 nearest neighbor  2.4   \n",
        "   LeNet1                1.7   \n",
        "   SVN                   1.1\n",
        "\n",
        "As result of both databases suggest that SVN generates the best result.\n",
        "\n",
        "There are many other areas where SVM can be used and all are listed below\n",
        "\n",
        "\n",
        "*   Classification of pictures can likewise be performed utilizing SVMs.\n",
        "*   The SVM algorithm has been broadly connected in the biology and different sciences.\n",
        "\n",
        "And i believe that in biology side SVM could use for classifying various diseases based on the existing data. So i believe this area suppose to be in depth of reasearch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzGK1ohBB-FS",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw8LfyvRB-FT",
        "colab_type": "text"
      },
      "source": [
        "The presentation of this paper is good in terms of standard of quality and structure. Main in the abstract it gives a clear view about for what paper is written but in the i feel that it stratched little bit longer than it should be. The body of the paper contain the depth knowledge of the proposed method with the ample amount of mathematical equation which explain in better way. Just author suppposed to add little graphical representation in each methods to understand reader easily. Also at the end few experiements were given of real time to demonstrate the proposed algorithm's value by comparing the result with other algorithms which is good. Thus, overall structure of the paper is clear and author tried to deliver knowledge effectively.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8aCf_ALB-FU",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Cortes, C. and Vapnik, V., 1995. Support-vector networks. Machine learning, 20(3), pp.273-297.\n",
        "\n",
        "Boser, B.E., Guyon, I.M. and Vapnik, V.N., 2003. A training algorithm for optimal margin classiÔ¨Åers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory (pp. 144-152).\n",
        "\n",
        "Fisher, R.A., 1936. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7(2), pp.179-188.\n",
        "\n",
        "Rosenblatt, F., 1962. Principles ofneurodynamics Spartan Books. NewYork, NY, USA."
      ]
    }
  ]
}